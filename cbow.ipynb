{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f9c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor, mean, unsqueeze, cat, transpose, matmul, sigmoid, arange, abs, optim, zeros\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9f0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reviews_Movies_and_TV.json\") as json_file:\n",
    "    file_contents = json_file.readlines()\n",
    "    \n",
    "no_of_reviews = len(file_contents)\n",
    "\n",
    "sentences = []\n",
    "words = []\n",
    "word_freq = {}\n",
    "count = 0\n",
    "MAX_SENTENCES = 40000\n",
    "\n",
    "for i in range(no_of_reviews):\n",
    "    review = json.loads(file_contents[i])['reviewText']\n",
    "    review_sent = sent_tokenize(review.lower())\n",
    "    review = re.sub(r'(?<=[a-zA-Z])(\\.\\?\\!)(?=[a-zA-Z])', '\\1 ', review)\n",
    "#     for i,review in enumerate(review_sent):\n",
    "#         review_sent[i] = re.sub(r'[\\.\\?\\!]', '', review)\n",
    "    sentences += review_sent\n",
    "    count += len(review_sent)\n",
    "    for sent in review_sent:\n",
    "        review_words = word_tokenize(sent)\n",
    "        words += review_words\n",
    "        for word in review_words:\n",
    "            try: word_freq[word] += 1\n",
    "            except KeyError: word_freq[word] = 1\n",
    "    if count >= MAX_SENTENCES:\n",
    "        break\n",
    "        \n",
    "freq_threshold = 3\n",
    "words = [word if word_freq[word] > freq_threshold else '<unk>' for word in words]\n",
    "\n",
    "sentence_list = sentences.copy()\n",
    "for i,sentence in enumerate(sentence_list):\n",
    "    ws = word_tokenize(sentence)\n",
    "    ws = [w if word_freq[w] > freq_threshold else '<unk>' for w in ws]\n",
    "    sentences[i] = ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c457d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for word in words:\n",
    "    try: word_freq[word] += 1\n",
    "    except KeyError: word_freq[word] = 0\n",
    "        \n",
    "vocab = list(word_freq.keys())\n",
    "\n",
    "index_to_word = {i: list(vocab)[i] for i in range(len(vocab)) }\n",
    "word_to_index = {word: i for i, word in index_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(self):\n",
    "    xTrain = []\n",
    "    yTrain = []\n",
    "    c = 0\n",
    "    with open(\"tokenised.txt\", \"r\") as f:\n",
    "        self.distribution = [0]*len(self.vocab)\n",
    "        total_words = 0\n",
    "        for line in f:\n",
    "            c += 1\n",
    "            words = eval(line)\n",
    "            total_words += len(words)\n",
    "            for word in words:\n",
    "                self.distribution[self.word_indices[word]] += 1\n",
    "\n",
    "            if (c % 1000 == 0):\n",
    "                print(\"line number\", c)\n",
    "            if (c == STOP_AT):\n",
    "                break\n",
    "\n",
    "        self.distribution = list(map(lambda x: x/total_words,\n",
    "                                     self.distribution))\n",
    "\n",
    "    print(\"Getting data\")\n",
    "    with open(\"tokenised.txt\", \"r\") as f:\n",
    "        #one_hots = F.one_hot(arange(len(self.vocab))).float()\n",
    "        z = zeros(len(self.vocab))\n",
    "        def one_hot(index):\n",
    "            x = z\n",
    "            x[index] = 1.\n",
    "            return x\n",
    "\n",
    "        c = 0\n",
    "        for line in f:\n",
    "            c += 1\n",
    "            words = eval(line)\n",
    "\n",
    "            window = words[:self.window_size*2+1]\n",
    "            for t, word in enumerate \\\n",
    "                             (words[self.window_size:-self.window_size]):\n",
    "                context = window.copy()\n",
    "                context.pop(self.window_size)\n",
    "                #context = words[t:t+self.window_size] \\\n",
    "                #        + words[t+self.window_size+1:t+self.window_size*2+1]\n",
    "                try: window = window[1:] + [words[t + self.window_size*2+1]]\n",
    "                except: pass\n",
    "                xTrain.append(cat\n",
    "                                (list(map(lambda w: unsqueeze(one_hot(self.word_indices[w]),0),\n",
    "                                           context +\n",
    "                                           [word] +\n",
    "                                           self.get_noise(context, word)))))\n",
    "                yTrain.append(tensor([1.] + [0.]*self.noise))\n",
    "\n",
    "            #if (c % 1000 == 0):\n",
    "            print(\"line number\", c)\n",
    "            if (c == STOP_AT):\n",
    "                break\n",
    "\n",
    "    self.trainset = ([t.float() for t in xTrain], [t.float() for t in yTrain])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
